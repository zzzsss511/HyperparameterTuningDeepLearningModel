{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load related library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scorer accuracy\n",
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = pd.read_csv(\"./Data/train.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = trainSet.drop(columns=['Name', 'PassengerId', 'Ticket', 'Cabin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count   Dtype  \n",
      "---  ------    --------------   -----  \n",
      " 0   Survived  100000 non-null  int64  \n",
      " 1   Pclass    100000 non-null  int64  \n",
      " 2   Sex       100000 non-null  object \n",
      " 3   Age       96708 non-null   float64\n",
      " 4   SibSp     100000 non-null  int64  \n",
      " 5   Parch     100000 non-null  int64  \n",
      " 6   Fare      99866 non-null   float64\n",
      " 7   Embarked  99750 non-null   object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96332 entries, 2 to 99999\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  96332 non-null  int64  \n",
      " 1   Pclass    96332 non-null  int64  \n",
      " 2   Sex       96332 non-null  object \n",
      " 3   Age       96332 non-null  float64\n",
      " 4   SibSp     96332 non-null  int64  \n",
      " 5   Parch     96332 non-null  int64  \n",
      " 6   Fare      96332 non-null  float64\n",
      " 7   Embarked  96332 non-null  object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy coding of the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, validation data set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train.drop(columns=\"Survived\", axis=0)\n",
    "                                                , train['Survived']\n",
    "                                                , test_size=0.2\n",
    "                                                , random_state=2023\n",
    "                                                , stratify=train['Survived'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters (**NOT including `layers`**)\n",
    "\n",
    "- `Optimizer`\n",
    "- `Activation`\n",
    "- `Neurons`\n",
    "- `Batch size`\n",
    "- `Epochs`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo1(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "        nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn = {\n",
    "    'neurons': (10, 100)\n",
    "  , 'activation': (0, 9) # `activationL` has 10 elements\n",
    "  , 'optimizer': (0, 7) # `optimizerL` has 8 elements\n",
    "  , 'learning_rate': (0.01, 1)\n",
    "  , 'batch_size': (200, 1000)\n",
    "  , \"epochs\": (20, 100)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 22:05:46.578805: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7675   \u001b[0m | \u001b[0m2.898    \u001b[0m | \u001b[0m912.3    \u001b[0m | \u001b[0m67.04    \u001b[0m | \u001b[0m0.1353   \u001b[0m | \u001b[0m22.72    \u001b[0m | \u001b[0m3.275    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.7641   \u001b[0m | \u001b[0m0.1988   \u001b[0m | \u001b[0m781.8    \u001b[0m | \u001b[0m61.95    \u001b[0m | \u001b[0m0.5495   \u001b[0m | \u001b[0m51.07    \u001b[0m | \u001b[0m3.51     \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.7279   \u001b[0m | \u001b[0m3.55     \u001b[0m | \u001b[0m320.9    \u001b[0m | \u001b[0m48.87    \u001b[0m | \u001b[0m0.1705   \u001b[0m | \u001b[0m40.42    \u001b[0m | \u001b[0m1.262    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.7687   \u001b[0m | \u001b[95m3.519    \u001b[0m | \u001b[95m228.5    \u001b[0m | \u001b[95m65.19    \u001b[0m | \u001b[95m0.2114   \u001b[0m | \u001b[95m38.85    \u001b[0m | \u001b[95m2.636    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.6947   \u001b[0m | \u001b[0m1.656    \u001b[0m | \u001b[0m283.2    \u001b[0m | \u001b[0m56.39    \u001b[0m | \u001b[0m0.2039   \u001b[0m | \u001b[0m44.07    \u001b[0m | \u001b[0m6.514    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.5719   \u001b[0m | \u001b[0m6.841    \u001b[0m | \u001b[0m816.6    \u001b[0m | \u001b[0m67.74    \u001b[0m | \u001b[0m0.7937   \u001b[0m | \u001b[0m82.93    \u001b[0m | \u001b[0m6.864    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.7581   \u001b[0m | \u001b[0m7.963    \u001b[0m | \u001b[0m287.8    \u001b[0m | \u001b[0m85.58    \u001b[0m | \u001b[0m0.3145   \u001b[0m | \u001b[0m33.53    \u001b[0m | \u001b[0m2.84     \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.6544   \u001b[0m | \u001b[0m4.981    \u001b[0m | \u001b[0m700.4    \u001b[0m | \u001b[0m26.3     \u001b[0m | \u001b[0m0.9726   \u001b[0m | \u001b[0m47.02    \u001b[0m | \u001b[0m5.052    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.7671   \u001b[0m | \u001b[0m5.97     \u001b[0m | \u001b[0m374.6    \u001b[0m | \u001b[0m34.97    \u001b[0m | \u001b[0m0.7325   \u001b[0m | \u001b[0m87.7     \u001b[0m | \u001b[0m2.742    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.7629   \u001b[0m | \u001b[0m0.9904   \u001b[0m | \u001b[0m930.2    \u001b[0m | \u001b[0m48.56    \u001b[0m | \u001b[0m0.4188   \u001b[0m | \u001b[0m26.52    \u001b[0m | \u001b[0m4.102    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.7602   \u001b[0m | \u001b[0m7.701    \u001b[0m | \u001b[0m831.7    \u001b[0m | \u001b[0m27.03    \u001b[0m | \u001b[0m0.9337   \u001b[0m | \u001b[0m55.0     \u001b[0m | \u001b[0m2.55     \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.7651   \u001b[0m | \u001b[0m4.352    \u001b[0m | \u001b[0m612.2    \u001b[0m | \u001b[0m99.57    \u001b[0m | \u001b[0m0.7559   \u001b[0m | \u001b[0m42.92    \u001b[0m | \u001b[0m4.28     \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.7668   \u001b[0m | \u001b[0m3.777    \u001b[0m | \u001b[0m273.3    \u001b[0m | \u001b[0m62.79    \u001b[0m | \u001b[0m0.3474   \u001b[0m | \u001b[0m11.65    \u001b[0m | \u001b[0m4.224    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.5739   \u001b[0m | \u001b[0m8.296    \u001b[0m | \u001b[0m425.1    \u001b[0m | \u001b[0m86.78    \u001b[0m | \u001b[0m0.5541   \u001b[0m | \u001b[0m11.96    \u001b[0m | \u001b[0m2.429    \u001b[0m |\n",
      "| \u001b[95m15       \u001b[0m | \u001b[95m0.7689   \u001b[0m | \u001b[95m4.95     \u001b[0m | \u001b[95m210.6    \u001b[0m | \u001b[95m77.27    \u001b[0m | \u001b[95m0.1979   \u001b[0m | \u001b[95m76.82    \u001b[0m | \u001b[95m4.406    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.7675   \u001b[0m | \u001b[0m3.26     \u001b[0m | \u001b[0m701.6    \u001b[0m | \u001b[0m61.19    \u001b[0m | \u001b[0m0.896    \u001b[0m | \u001b[0m56.68    \u001b[0m | \u001b[0m4.252    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.7516   \u001b[0m | \u001b[0m4.273    \u001b[0m | \u001b[0m767.7    \u001b[0m | \u001b[0m29.22    \u001b[0m | \u001b[0m0.7355   \u001b[0m | \u001b[0m89.02    \u001b[0m | \u001b[0m3.633    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.5144   \u001b[0m | \u001b[0m6.124    \u001b[0m | \u001b[0m548.4    \u001b[0m | \u001b[0m97.74    \u001b[0m | \u001b[0m0.3715   \u001b[0m | \u001b[0m66.04    \u001b[0m | \u001b[0m1.808    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.7622   \u001b[0m | \u001b[0m6.132    \u001b[0m | \u001b[0m901.3    \u001b[0m | \u001b[0m39.38    \u001b[0m | \u001b[0m0.1512   \u001b[0m | \u001b[0m84.29    \u001b[0m | \u001b[0m3.258    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.7098   \u001b[0m | \u001b[0m3.044    \u001b[0m | \u001b[0m946.9    \u001b[0m | \u001b[0m56.35    \u001b[0m | \u001b[0m0.4993   \u001b[0m | \u001b[0m14.13    \u001b[0m | \u001b[0m0.8328   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.5144   \u001b[0m | \u001b[0m4.842    \u001b[0m | \u001b[0m243.8    \u001b[0m | \u001b[0m44.04    \u001b[0m | \u001b[0m0.3843   \u001b[0m | \u001b[0m64.81    \u001b[0m | \u001b[0m5.767    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.5719   \u001b[0m | \u001b[0m1.646    \u001b[0m | \u001b[0m829.5    \u001b[0m | \u001b[0m44.91    \u001b[0m | \u001b[0m0.7738   \u001b[0m | \u001b[0m92.35    \u001b[0m | \u001b[0m1.584    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.5719   \u001b[0m | \u001b[0m8.92     \u001b[0m | \u001b[0m449.2    \u001b[0m | \u001b[0m90.85    \u001b[0m | \u001b[0m0.6967   \u001b[0m | \u001b[0m14.91    \u001b[0m | \u001b[0m1.413    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.724    \u001b[0m | \u001b[0m7.732    \u001b[0m | \u001b[0m728.6    \u001b[0m | \u001b[0m89.82    \u001b[0m | \u001b[0m0.7265   \u001b[0m | \u001b[0m18.27    \u001b[0m | \u001b[0m0.1538   \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.5431   \u001b[0m | \u001b[0m1.638    \u001b[0m | \u001b[0m420.9    \u001b[0m | \u001b[0m73.87    \u001b[0m | \u001b[0m0.5748   \u001b[0m | \u001b[0m43.96    \u001b[0m | \u001b[0m1.992    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.7626   \u001b[0m | \u001b[0m3.822    \u001b[0m | \u001b[0m211.5    \u001b[0m | \u001b[0m98.39    \u001b[0m | \u001b[0m0.03529  \u001b[0m | \u001b[0m37.03    \u001b[0m | \u001b[0m0.8911   \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.767    \u001b[0m | \u001b[0m8.798    \u001b[0m | \u001b[0m875.6    \u001b[0m | \u001b[0m29.05    \u001b[0m | \u001b[0m0.1935   \u001b[0m | \u001b[0m36.78    \u001b[0m | \u001b[0m5.099    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.5719   \u001b[0m | \u001b[0m6.868    \u001b[0m | \u001b[0m901.8    \u001b[0m | \u001b[0m36.31    \u001b[0m | \u001b[0m0.07918  \u001b[0m | \u001b[0m85.92    \u001b[0m | \u001b[0m0.5771   \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.7255   \u001b[0m | \u001b[0m0.607    \u001b[0m | \u001b[0m888.6    \u001b[0m | \u001b[0m68.44    \u001b[0m | \u001b[0m0.5056   \u001b[0m | \u001b[0m21.14    \u001b[0m | \u001b[0m5.445    \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "nn_bo1 = BayesianOptimization(nn_cl_bo1, params_nn, random_state=2023)\n",
    "nn_bo1.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'selu',\n",
       " 'batch_size': 210.56450490241733,\n",
       " 'epochs': 77,\n",
       " 'learning_rate': 0.19787923530180895,\n",
       " 'neurons': 76.82203661959858,\n",
       " 'optimizer': 'Adagrad',\n",
       " 'bach_size': 211}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_1 = nn_bo1.max['params']\n",
    "\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_1['activation'] = activationL[round(params_nn_1['activation'])]\n",
    "\n",
    "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "params_nn_1['optimizer'] = optimizerL[round(params_nn_1['optimizer'])]\n",
    "\n",
    "params_nn_1['bach_size'] = round(params_nn_1['batch_size'])\n",
    "params_nn_1['epochs'] = round(params_nn_1['epochs'])\n",
    "\n",
    "params_nn_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters (**Including `layers`**)\n",
    "\n",
    "- `Optimizer`\n",
    "- `Activation`\n",
    "- `Neurons`\n",
    "- `Batch size`\n",
    "- `Epochs`\n",
    "- `Layers`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo2(neurons, activation, optimizer, learning_rate,  batch_size, epochs, layers1, layers2, normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "        if normalization > 0.5:\n",
    "            nn.add(BatchNormalization())\n",
    "\n",
    "        for i in range(layers1):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "\n",
    "        if dropout > 0.5:\n",
    "            nn.add(Dropout(dropout_rate, seed=2023))\n",
    "\n",
    "        for i in range(layers2):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nn2 ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(200, 1000),\n",
    "    'epochs':(20, 100),\n",
    "    'layers1':(1,3),\n",
    "    'layers2':(1,3),\n",
    "    'normalization':(0,1),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  | learni... |  neurons  | normal... | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7675   \u001b[0m | \u001b[0m2.898    \u001b[0m | \u001b[0m912.3    \u001b[0m | \u001b[0m0.5881   \u001b[0m | \u001b[0m0.03798  \u001b[0m | \u001b[0m31.31    \u001b[0m | \u001b[0m1.936    \u001b[0m | \u001b[0m1.044    \u001b[0m | \u001b[0m0.73     \u001b[0m | \u001b[0m57.19    \u001b[0m | \u001b[0m0.5449   \u001b[0m | \u001b[0m3.195    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m0.7527   \u001b[0m | \u001b[0m4.512    \u001b[0m | \u001b[0m515.6    \u001b[0m | \u001b[0m0.1512   \u001b[0m | \u001b[0m0.1083   \u001b[0m | \u001b[0m32.97    \u001b[0m | \u001b[0m1.676    \u001b[0m | \u001b[0m1.361    \u001b[0m | \u001b[0m0.3971   \u001b[0m | \u001b[0m13.21    \u001b[0m | \u001b[0m0.5649   \u001b[0m | \u001b[0m1.424    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.7584   \u001b[0m | \u001b[0m2.885    \u001b[0m | \u001b[0m501.3    \u001b[0m | \u001b[0m0.1841   \u001b[0m | \u001b[0m0.03119  \u001b[0m | \u001b[0m56.39    \u001b[0m | \u001b[0m1.392    \u001b[0m | \u001b[0m1.757    \u001b[0m | \u001b[0m0.9312   \u001b[0m | \u001b[0m78.41    \u001b[0m | \u001b[0m0.7708   \u001b[0m | \u001b[0m4.177    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.5719   \u001b[0m | \u001b[0m7.125    \u001b[0m | \u001b[0m848.3    \u001b[0m | \u001b[0m0.9806   \u001b[0m | \u001b[0m0.2654   \u001b[0m | \u001b[0m28.78    \u001b[0m | \u001b[0m2.639    \u001b[0m | \u001b[0m1.615    \u001b[0m | \u001b[0m0.2689   \u001b[0m | \u001b[0m46.52    \u001b[0m | \u001b[0m0.5534   \u001b[0m | \u001b[0m4.379    \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.6931   \u001b[0m | \u001b[0m0.7088   \u001b[0m | \u001b[0m977.8    \u001b[0m | \u001b[0m0.4113   \u001b[0m | \u001b[0m0.2165   \u001b[0m | \u001b[0m73.06    \u001b[0m | \u001b[0m1.436    \u001b[0m | \u001b[0m1.374    \u001b[0m | \u001b[0m0.7325   \u001b[0m | \u001b[0m87.7     \u001b[0m | \u001b[0m0.3917   \u001b[0m | \u001b[0m0.7703   \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.7652   \u001b[0m | \u001b[0m8.215    \u001b[0m | \u001b[0m485.6    \u001b[0m | \u001b[0m0.413    \u001b[0m | \u001b[0m0.05506  \u001b[0m | \u001b[0m66.88    \u001b[0m | \u001b[0m2.711    \u001b[0m | \u001b[0m2.579    \u001b[0m | \u001b[0m0.09696  \u001b[0m | \u001b[0m93.97    \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m2.55     \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.4856   \u001b[0m | \u001b[0m4.352    \u001b[0m | \u001b[0m612.2    \u001b[0m | \u001b[0m0.9946   \u001b[0m | \u001b[0m0.226    \u001b[0m | \u001b[0m49.27    \u001b[0m | \u001b[0m2.223    \u001b[0m | \u001b[0m1.839    \u001b[0m | \u001b[0m0.1008   \u001b[0m | \u001b[0m58.14    \u001b[0m | \u001b[0m0.3408   \u001b[0m | \u001b[0m0.1284   \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m0.7589   \u001b[0m | \u001b[0m5.431    \u001b[0m | \u001b[0m937.4    \u001b[0m | \u001b[0m0.2814   \u001b[0m | \u001b[0m0.2504   \u001b[0m | \u001b[0m63.97    \u001b[0m | \u001b[0m1.044    \u001b[0m | \u001b[0m1.694    \u001b[0m | \u001b[0m0.5545   \u001b[0m | \u001b[0m11.19    \u001b[0m | \u001b[0m0.7159   \u001b[0m | \u001b[0m1.328    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.5719   \u001b[0m | \u001b[0m6.682    \u001b[0m | \u001b[0m703.5    \u001b[0m | \u001b[0m0.3622   \u001b[0m | \u001b[0m0.1881   \u001b[0m | \u001b[0m61.19    \u001b[0m | \u001b[0m2.79     \u001b[0m | \u001b[0m2.037    \u001b[0m | \u001b[0m0.6113   \u001b[0m | \u001b[0m52.73    \u001b[0m | \u001b[0m0.7096   \u001b[0m | \u001b[0m0.8067   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.5719   \u001b[0m | \u001b[0m6.595    \u001b[0m | \u001b[0m902.4    \u001b[0m | \u001b[0m0.5191   \u001b[0m | \u001b[0m0.2041   \u001b[0m | \u001b[0m54.84    \u001b[0m | \u001b[0m2.944    \u001b[0m | \u001b[0m1.73     \u001b[0m | \u001b[0m0.6265   \u001b[0m | \u001b[0m33.25    \u001b[0m | \u001b[0m0.6813   \u001b[0m | \u001b[0m6.136    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.7658   \u001b[0m | \u001b[0m2.18     \u001b[0m | \u001b[0m314.1    \u001b[0m | \u001b[0m0.8254   \u001b[0m | \u001b[0m0.1396   \u001b[0m | \u001b[0m47.05    \u001b[0m | \u001b[0m2.867    \u001b[0m | \u001b[0m1.909    \u001b[0m | \u001b[0m0.4993   \u001b[0m | \u001b[0m14.13    \u001b[0m | \u001b[0m0.119    \u001b[0m | \u001b[0m3.766    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.5143   \u001b[0m | \u001b[0m0.4931   \u001b[0m | \u001b[0m440.4    \u001b[0m | \u001b[0m0.3781   \u001b[0m | \u001b[0m0.1827   \u001b[0m | \u001b[0m85.91    \u001b[0m | \u001b[0m1.366    \u001b[0m | \u001b[0m2.574    \u001b[0m | \u001b[0m0.3183   \u001b[0m | \u001b[0m79.44    \u001b[0m | \u001b[0m0.9149   \u001b[0m | \u001b[0m1.584    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.7564   \u001b[0m | \u001b[0m8.92     \u001b[0m | \u001b[0m449.2    \u001b[0m | \u001b[0m0.8856   \u001b[0m | \u001b[0m0.2081   \u001b[0m | \u001b[0m24.36    \u001b[0m | \u001b[0m1.404    \u001b[0m | \u001b[0m2.718    \u001b[0m | \u001b[0m0.6641   \u001b[0m | \u001b[0m88.55    \u001b[0m | \u001b[0m0.7237   \u001b[0m | \u001b[0m0.6436   \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.6091   \u001b[0m | \u001b[0m0.1977   \u001b[0m | \u001b[0m345.6    \u001b[0m | \u001b[0m0.2761   \u001b[0m | \u001b[0m0.202    \u001b[0m | \u001b[0m65.64    \u001b[0m | \u001b[0m1.755    \u001b[0m | \u001b[0m1.569    \u001b[0m | \u001b[0m0.2106   \u001b[0m | \u001b[0m39.78    \u001b[0m | \u001b[0m0.3645   \u001b[0m | \u001b[0m1.418    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.5441   \u001b[0m | \u001b[0m5.427    \u001b[0m | \u001b[0m250.4    \u001b[0m | \u001b[0m0.7315   \u001b[0m | \u001b[0m0.2202   \u001b[0m | \u001b[0m67.21    \u001b[0m | \u001b[0m2.434    \u001b[0m | \u001b[0m2.426    \u001b[0m | \u001b[0m0.7781   \u001b[0m | \u001b[0m71.56    \u001b[0m | \u001b[0m0.7979   \u001b[0m | \u001b[0m4.123    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.7634   \u001b[0m | \u001b[0m7.737    \u001b[0m | \u001b[0m553.8    \u001b[0m | \u001b[0m0.388    \u001b[0m | \u001b[0m0.08434  \u001b[0m | \u001b[0m27.63    \u001b[0m | \u001b[0m1.867    \u001b[0m | \u001b[0m2.352    \u001b[0m | \u001b[0m0.2582   \u001b[0m | \u001b[0m40.57    \u001b[0m | \u001b[0m0.6354   \u001b[0m | \u001b[0m4.767    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.7633   \u001b[0m | \u001b[0m3.152    \u001b[0m | \u001b[0m604.8    \u001b[0m | \u001b[0m0.4119   \u001b[0m | \u001b[0m0.2227   \u001b[0m | \u001b[0m31.6     \u001b[0m | \u001b[0m2.462    \u001b[0m | \u001b[0m2.031    \u001b[0m | \u001b[0m0.3393   \u001b[0m | \u001b[0m93.31    \u001b[0m | \u001b[0m0.9734   \u001b[0m | \u001b[0m3.917    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.5432   \u001b[0m | \u001b[0m0.917    \u001b[0m | \u001b[0m737.7    \u001b[0m | \u001b[0m0.9307   \u001b[0m | \u001b[0m0.08556  \u001b[0m | \u001b[0m29.43    \u001b[0m | \u001b[0m2.701    \u001b[0m | \u001b[0m2.391    \u001b[0m | \u001b[0m0.9006   \u001b[0m | \u001b[0m94.77    \u001b[0m | \u001b[0m0.5163   \u001b[0m | \u001b[0m4.864    \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.7232   \u001b[0m | \u001b[0m3.883    \u001b[0m | \u001b[0m899.2    \u001b[0m | \u001b[0m0.6551   \u001b[0m | \u001b[0m0.02753  \u001b[0m | \u001b[0m43.47    \u001b[0m | \u001b[0m1.594    \u001b[0m | \u001b[0m1.667    \u001b[0m | \u001b[0m0.1267   \u001b[0m | \u001b[0m63.06    \u001b[0m | \u001b[0m0.4873   \u001b[0m | \u001b[0m0.7257   \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.5432   \u001b[0m | \u001b[0m2.648    \u001b[0m | \u001b[0m701.7    \u001b[0m | \u001b[0m0.07346  \u001b[0m | \u001b[0m0.05054  \u001b[0m | \u001b[0m76.62    \u001b[0m | \u001b[0m2.189    \u001b[0m | \u001b[0m2.029    \u001b[0m | \u001b[0m0.54     \u001b[0m | \u001b[0m33.33    \u001b[0m | \u001b[0m0.6661   \u001b[0m | \u001b[0m0.1974   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.5431   \u001b[0m | \u001b[0m6.158    \u001b[0m | \u001b[0m667.3    \u001b[0m | \u001b[0m0.5589   \u001b[0m | \u001b[0m0.04195  \u001b[0m | \u001b[0m30.23    \u001b[0m | \u001b[0m2.089    \u001b[0m | \u001b[0m2.132    \u001b[0m | \u001b[0m0.8174   \u001b[0m | \u001b[0m41.98    \u001b[0m | \u001b[0m0.5924   \u001b[0m | \u001b[0m4.727    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.5144   \u001b[0m | \u001b[0m0.8156   \u001b[0m | \u001b[0m783.9    \u001b[0m | \u001b[0m0.7164   \u001b[0m | \u001b[0m0.2642   \u001b[0m | \u001b[0m53.48    \u001b[0m | \u001b[0m1.438    \u001b[0m | \u001b[0m2.371    \u001b[0m | \u001b[0m0.9643   \u001b[0m | \u001b[0m61.84    \u001b[0m | \u001b[0m0.2376   \u001b[0m | \u001b[0m2.385    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.7628   \u001b[0m | \u001b[0m5.679    \u001b[0m | \u001b[0m741.3    \u001b[0m | \u001b[0m0.6444   \u001b[0m | \u001b[0m0.201    \u001b[0m | \u001b[0m66.52    \u001b[0m | \u001b[0m1.881    \u001b[0m | \u001b[0m2.203    \u001b[0m | \u001b[0m0.3842   \u001b[0m | \u001b[0m33.18    \u001b[0m | \u001b[0m0.9529   \u001b[0m | \u001b[0m4.114    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.5719   \u001b[0m | \u001b[0m7.418    \u001b[0m | \u001b[0m594.4    \u001b[0m | \u001b[0m0.681    \u001b[0m | \u001b[0m0.1518   \u001b[0m | \u001b[0m38.07    \u001b[0m | \u001b[0m1.69     \u001b[0m | \u001b[0m2.559    \u001b[0m | \u001b[0m0.1059   \u001b[0m | \u001b[0m83.26    \u001b[0m | \u001b[0m0.1024   \u001b[0m | \u001b[0m4.339    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.5144   \u001b[0m | \u001b[0m0.6685   \u001b[0m | \u001b[0m976.4    \u001b[0m | \u001b[0m0.6527   \u001b[0m | \u001b[0m0.2858   \u001b[0m | \u001b[0m76.45    \u001b[0m | \u001b[0m1.211    \u001b[0m | \u001b[0m1.785    \u001b[0m | \u001b[0m0.7721   \u001b[0m | \u001b[0m61.96    \u001b[0m | \u001b[0m0.3429   \u001b[0m | \u001b[0m0.1709   \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.7675   \u001b[0m | \u001b[0m2.87     \u001b[0m | \u001b[0m906.1    \u001b[0m | \u001b[0m0.4514   \u001b[0m | \u001b[0m0.1315   \u001b[0m | \u001b[0m58.15    \u001b[0m | \u001b[0m1.011    \u001b[0m | \u001b[0m2.201    \u001b[0m | \u001b[0m0.2761   \u001b[0m | \u001b[0m26.25    \u001b[0m | \u001b[0m0.7999   \u001b[0m | \u001b[0m3.088    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.5387   \u001b[0m | \u001b[0m3.601    \u001b[0m | \u001b[0m244.7    \u001b[0m | \u001b[0m0.7623   \u001b[0m | \u001b[0m0.1214   \u001b[0m | \u001b[0m31.29    \u001b[0m | \u001b[0m1.259    \u001b[0m | \u001b[0m2.692    \u001b[0m | \u001b[0m0.3889   \u001b[0m | \u001b[0m67.01    \u001b[0m | \u001b[0m0.5368   \u001b[0m | \u001b[0m0.6861   \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.7662   \u001b[0m | \u001b[0m8.387    \u001b[0m | \u001b[0m533.1    \u001b[0m | \u001b[0m0.9818   \u001b[0m | \u001b[0m0.2767   \u001b[0m | \u001b[0m71.31    \u001b[0m | \u001b[0m1.625    \u001b[0m | \u001b[0m2.701    \u001b[0m | \u001b[0m0.9549   \u001b[0m | \u001b[0m79.94    \u001b[0m | \u001b[0m0.9583   \u001b[0m | \u001b[0m4.286    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.5814   \u001b[0m | \u001b[0m1.317    \u001b[0m | \u001b[0m663.9    \u001b[0m | \u001b[0m0.4203   \u001b[0m | \u001b[0m0.2652   \u001b[0m | \u001b[0m28.76    \u001b[0m | \u001b[0m1.907    \u001b[0m | \u001b[0m2.138    \u001b[0m | \u001b[0m0.4453   \u001b[0m | \u001b[0m36.61    \u001b[0m | \u001b[0m0.8668   \u001b[0m | \u001b[0m1.048    \u001b[0m |\n",
      "=============================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "nn_bo2 = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=2023)\n",
    "nn_bo2.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'softsign',\n",
       " 'batch_size': 912,\n",
       " 'dropout': 1,\n",
       " 'dropout_rate': 0.03797882805128737,\n",
       " 'epochs': 31,\n",
       " 'layers1': 2,\n",
       " 'layers2': 1,\n",
       " 'learning_rate': 0.7300019655294788,\n",
       " 'neurons': 57,\n",
       " 'normalization': 1,\n",
       " 'optimizer': <keras.optimizer_v2.adadelta.Adadelta at 0x7f9946340be0>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_2 = nn_bo2.max['params']\n",
    "\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_2['activation'] = activationL[round(params_nn_2['activation'])]\n",
    "\n",
    "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "learning_rate = params_nn_2['learning_rate']\n",
    "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "params_nn_2['optimizer'] = optimizerD[optimizerL[round(params_nn_2['optimizer'])]]\n",
    "\n",
    "params_nn_2['batch_size'] = round(params_nn_2['batch_size'])\n",
    "params_nn_2['epochs'] = round(params_nn_2['epochs'])\n",
    "params_nn_2['layers1'] = round(params_nn_2['layers1'])\n",
    "params_nn_2['layers2'] = round(params_nn_2['layers2'])\n",
    "params_nn_2['neurons'] = round(params_nn_2['neurons'])\n",
    "params_nn_2['dropout'] = round(params_nn_2['dropout'])\n",
    "params_nn_2['normalization'] = round(params_nn_2['normalization'])\n",
    "\n",
    "params_nn_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and fit the model using the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cl_fun_final():\n",
    "    nn_final = Sequential()\n",
    "    nn_final.add(Dense(params_nn_2['neurons'], input_dim=10, activation=params_nn_2['activation']))\n",
    "    if params_nn_2['normalization'] > 0.5:\n",
    "        nn_final.add(BatchNormalization())\n",
    "\n",
    "    for i in range(params_nn_2['layers1']):\n",
    "        nn_final.add(Dense(params_nn_2['neurons'], activation=params_nn_2['activation']))\n",
    "\n",
    "    if params_nn_2['dropout'] > 0.5:\n",
    "        nn_final.add(Dropout(params_nn_2['dropout_rate'], seed=2023))\n",
    "\n",
    "    for i in range(params_nn_2['layers2']):\n",
    "        nn_final.add(Dense(params_nn_2['neurons'], activation=params_nn_2['activation']))\n",
    "\n",
    "    nn_final.add(Dense(1, activation='sigmoid'))\n",
    "    nn_final.compile(loss='binary_crossentropy', optimizer=params_nn_2['optimizer'], metrics=['accuracy'])\n",
    "\n",
    "    return nn_final\n",
    "\n",
    "es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "nn = KerasClassifier(build_fn=nn_cl_fun_final, epochs=params_nn_2['epochs'], batch_size=params_nn_2['batch_size'],\n",
    "                         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/31\n",
      "85/85 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.6954 - val_loss: 0.5835 - val_accuracy: 0.6719\n",
      "Epoch 2/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.7605 - val_loss: 0.5258 - val_accuracy: 0.7527\n",
      "Epoch 3/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7633 - val_loss: 0.5158 - val_accuracy: 0.7547\n",
      "Epoch 4/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.5013 - accuracy: 0.7655 - val_loss: 0.5046 - val_accuracy: 0.7648\n",
      "Epoch 5/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4999 - accuracy: 0.7661 - val_loss: 0.5055 - val_accuracy: 0.7662\n",
      "Epoch 6/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4988 - accuracy: 0.7675 - val_loss: 0.5013 - val_accuracy: 0.7677\n",
      "Epoch 7/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4983 - accuracy: 0.7671 - val_loss: 0.5038 - val_accuracy: 0.7633\n",
      "Epoch 8/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4976 - accuracy: 0.7673 - val_loss: 0.5004 - val_accuracy: 0.7677\n",
      "Epoch 9/31\n",
      "85/85 [==============================] - 0s 5ms/step - loss: 0.4969 - accuracy: 0.7681 - val_loss: 0.5054 - val_accuracy: 0.7621\n",
      "Epoch 10/31\n",
      "85/85 [==============================] - 0s 4ms/step - loss: 0.4968 - accuracy: 0.7681 - val_loss: 0.4965 - val_accuracy: 0.7679\n",
      "Epoch 11/31\n",
      "85/85 [==============================] - 0s 4ms/step - loss: 0.4964 - accuracy: 0.7672 - val_loss: 0.4933 - val_accuracy: 0.7699\n",
      "Epoch 12/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7680 - val_loss: 0.5017 - val_accuracy: 0.7677\n",
      "Epoch 13/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4956 - accuracy: 0.7677 - val_loss: 0.5105 - val_accuracy: 0.7514\n",
      "Epoch 14/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4952 - accuracy: 0.7694 - val_loss: 0.4935 - val_accuracy: 0.7664\n",
      "Epoch 15/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4948 - accuracy: 0.7686 - val_loss: 0.5053 - val_accuracy: 0.7611\n",
      "Epoch 16/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4948 - accuracy: 0.7686 - val_loss: 0.4933 - val_accuracy: 0.7712\n",
      "Epoch 17/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4942 - accuracy: 0.7692 - val_loss: 0.5016 - val_accuracy: 0.7605\n",
      "Epoch 18/31\n",
      "85/85 [==============================] - 0s 4ms/step - loss: 0.4948 - accuracy: 0.7693 - val_loss: 0.4988 - val_accuracy: 0.7659\n",
      "Epoch 19/31\n",
      "85/85 [==============================] - 0s 4ms/step - loss: 0.4944 - accuracy: 0.7692 - val_loss: 0.4926 - val_accuracy: 0.7712\n",
      "Epoch 20/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.7690 - val_loss: 0.5007 - val_accuracy: 0.7616\n",
      "Epoch 21/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4934 - accuracy: 0.7699 - val_loss: 0.4920 - val_accuracy: 0.7725\n",
      "Epoch 22/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4932 - accuracy: 0.7698 - val_loss: 0.4952 - val_accuracy: 0.7682\n",
      "Epoch 23/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7690 - val_loss: 0.4925 - val_accuracy: 0.7701\n",
      "Epoch 24/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7684 - val_loss: 0.4935 - val_accuracy: 0.7693\n",
      "Epoch 25/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4933 - accuracy: 0.7694 - val_loss: 0.4921 - val_accuracy: 0.7688\n",
      "Epoch 26/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4931 - accuracy: 0.7694 - val_loss: 0.4927 - val_accuracy: 0.7692\n",
      "Epoch 27/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.7701 - val_loss: 0.4927 - val_accuracy: 0.7705\n",
      "Epoch 28/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4928 - accuracy: 0.7694 - val_loss: 0.4927 - val_accuracy: 0.7700\n",
      "Epoch 29/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4923 - accuracy: 0.7693 - val_loss: 0.4971 - val_accuracy: 0.7664\n",
      "Epoch 30/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4925 - accuracy: 0.7693 - val_loss: 0.4951 - val_accuracy: 0.7667\n",
      "Epoch 31/31\n",
      "85/85 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7699 - val_loss: 0.4909 - val_accuracy: 0.7723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9933683e50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, validation_data= (X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6e4d5ef8480bde448feb7aa7993cda4fb0b67d180e6017422e62b472b016cda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
